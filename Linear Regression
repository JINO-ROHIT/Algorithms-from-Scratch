#FDefining our own dataset
def synthetic_data(w,b,num_examples):
  x = tf.zeros((num_examples,w.shape[0]))
  x += tf.random.normal(shape= x.shape)
  y = tf.matmul(x, tf.reshape(w,(-1,1))) + b
  #print(x.shape)
  y += tf.random.normal(shape = y.shape, stddev = 0.01)
  #print(y.shape)
  y = tf.reshape(y, (-1,1))
  #print(y.shape)
  return x,y

true_w = tf.constant([2,-3.4])
true_b = 4.2
features, labels = synthetic_data(true_w,true_b, 1000)

features[0],labels[0]
plt.scatter(features[:,1].numpy(),labels.numpy())  #thde linear relation

def data_iteration(batch_size, features, labels):
  num_examples = len(features)
  indices = list(range(num_examples))
  random.shuffle(indices)

  for i in range(0,num_examples,batch_size):
    j = tf.constant(indices[i:min(i + batch_size, num_examples)])
    yield tf.gather(features,j), tf.gather(labels,j)
    
batch_size = 10
for x,y in data_iteration(batch_size,features,labels):
  print(x,y)
  print('------------')

#parameters initialization
w = tf.Variable(tf.random.normal(shape= (2,1),mean = 0, stddev = 0.01),
                trainable = True)
b = tf.Variable(tf.zeros(1),trainable= True)

def linreg(x,w,b):
  return tf.matmul(x,w) + b
  
def squared_loss(y_hat,y):
  return (y_hat - tf.reshape(y, y_hat.shape)) **2 /2

def sgd(params, grads, lr, batch_size):
  for param , grad in zip(params,grads):
    param.assign_sub(lr*grad / batch_size)

lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
  for x,y in data_iteration(batch_size, features, labels):
    with tf.GradientTape() as g:
      l = loss(net(x,w,b), y)
    dw, db = g.gradient(l, [w,b])
    sgd([w,b] , [dw,db], lr, batch_size)
  train_l = loss(net(features, w, b), labels)
  print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}')
  
  
print(f'error in estimating w: {true_w - tf.reshape(w, true_w.shape)}')
print(f'error in estimating b: {true_b - b}')



